{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuración en Google Drive\n",
    "- Ejecutar las siguientes celdas si se trabajará con el presente Jupyter Notebook en Google Drive.\n",
    "- En caso de ejecutarlo en una PC local, omitir estas celdas.\n",
    "- Clonar el repositorio de GitHub en su computadora local.\n",
    "- Subir a Google Drive el directorio \"project\" que se encuentra en la carpeta \"ceia_uba/part_2\".\n",
    "\n",
    "De esta manera, toda ejecución en el Jupyter Notebook que produzca archivos de salida serán almacenados en las carpetas internas del proyecto de forma consistente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrucciones\n",
    "\n",
    "1) Instalar paquetes necesarios a través de pip.\n",
    "2) Importar paquetes necesarios.\n",
    "3) Montar Google Drive.\n",
    "4) Definir rutas dentro del proyecto.\n",
    "5) Importar paquetes propios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install numpy scipy matplotlib pandas\n",
    "!pip install torch torchvision torchaudio torchsummary\n",
    "!pip install pytorch-lightning\n",
    "!pip install opencv-python\n",
    "!pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = \"/content/drive/MyDrive/project/\"\n",
    "TRAIN_PATH = ROOT_PATH + \"dataset/train/\"\n",
    "VALID_PATH = ROOT_PATH + \"dataset/valid/\"\n",
    "TEST_PATH = ROOT_PATH + \"dataset/test/\"\n",
    "FIGURES_PATH = ROOT_PATH + \"figures/\"\n",
    "MODELS_PATH = ROOT_PATH + \"models/\"\n",
    "METRICS_PATH = ROOT_PATH + \"metrics/\"\n",
    "SOURCE_PATH = ROOT_PATH + \"src/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(SOURCE_PATH)\n",
    "from src.dataset import *\n",
    "from src.cnn import *\n",
    "from src.encoder import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuración en PC local\n",
    "- Ejecutar las siguientes celdas si se trabajará con el presente Jupyter Notebook en una PC local.\n",
    "- En caso de ejecutarlo en Google Drive, omitir estas celdas.\n",
    "- Clonar el repositorio de GitHub en su computadora local.\n",
    "- Abrir el proyecto con su entorno de desarrollo (e.g. VSCode).\n",
    "\n",
    "De esta manera, toda ejecución en el Jupyter Notebook que produzca archivos de salida serán almacenados en las carpetas internas del proyecto de forma consistente, dentro de la carpeta \"project\", localmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrucciones\n",
    "\n",
    "1) Instalar paquetes necesarios a través de pip (ejecutar las intrucciones para crear el entorno virtual). Los paquetes necesarios se encuentran en el archivo *requirements.txt*. Las intrucciones se encuentran en el archivo *README.md*.\n",
    "2) Importar paquetes necesarios.\n",
    "3) Importar paquetes propios.\n",
    "4) Definir rutas dentro del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.dataset import *\n",
    "from src.cnn import *\n",
    "from src.encoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = os.getcwd()\n",
    "TRAIN_PATH = ROOT_PATH + \"/dataset/train/\"\n",
    "VALID_PATH = ROOT_PATH + \"/dataset/valid/\"\n",
    "TEST_PATH = ROOT_PATH + \"/dataset/test/\"\n",
    "FIGURES_PATH = ROOT_PATH + \"/figures/\"\n",
    "MODELS_PATH = ROOT_PATH + \"/models/\"\n",
    "METRICS_PATH = ROOT_PATH + \"/metrics/\"\n",
    "SOURCE_PATH = ROOT_PATH + \"/src/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establecer el dispositivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando MPS (Apple GPU)\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Usando MPS (Apple GPU)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Usando CUDA (NVIDIA GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Usando CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Base\n",
    "El modelo base consiste en una red neuronal con cuatro capas convolucionales, seguido de una capa *fully connected*.\n",
    "\n",
    "Para cada bloque convolucional se puede establecer (por el usuario):\n",
    "- Canales de entrada.\n",
    "- Canales de salida.\n",
    "- Tamaño del kernel, stride y padding.\n",
    "- Modo del padding, i.e. como completar los píxeles extras.\n",
    "- Función de activación.\n",
    "- Tamaño del kernel, stride y padding en la capa de Max Pooling.\n",
    "\n",
    "Para la clase de la red neuronal convolucional se peude establecer (por el usuario):\n",
    "- La forma de la imagen de entrada (C, H, W), donde C es el número de canales, H es la altura y W es el ancho, en píxeles para estos últimos dos.\n",
    "- El número de clases a clasificar.\n",
    "- El número de neuronas a la salida de la capa *fully connected*.\n",
    "- La función costo.\n",
    "- La tasa de *dropout*.\n",
    "- El tipo de inicialización de los pesos en la red.\n",
    "- El dispositivo a utilizar, e.g. *cpu*, *gpu*, etc.\n",
    "\n",
    "## Variantes\n",
    "El Dataset elegido consiste en cartas de Poker. Contiene un total de 53 clases, con 7624 imágenes de entrenamiento, 265 de validación y 265 de prueba. Todas las imágenes son en color (RGB, 3 canales) y su dimensión es de 224 x 224 píxeles.\n",
    "\n",
    "### Data Augmentation\n",
    "Si bien el dataset tiene imágenes diversas dentro de cada clase, aproximadamente se tiene 144 imágenes por cada clase. Resulta en una aproximación pues la distribución no es exactamente uniforme. Luego de explorar visualmente el dataset, se observa que la mayoría de las cartas son monocromáticas, es decir, de un solo color. Por ejemplo, hay cartas solo de color rojo o negro, con fondo blanco. Las variaciones que se observan son, entre otras: giros, recortes, cambios de perspectiva, simbología extra, temáticas, etc. Sin embargo, se propone aumentar el dataset medianta *data augmentation*. Las transformaciones a realizar son:\n",
    "- Desplazamiento, escalado y rotación. Las regiones vacías se rellenan con el color negro y los giros oscilan entre +/- 15 grados.\n",
    "- Variación aleatoria en el brillo y contraste.\n",
    "- Volteo horizontal.\n",
    "\n",
    "El proceso de *data augmentation* se peude realizar la cantidad de veces que uno desea. Por ejemplo, al realizar este proceso una sola vez, el dataset se duplica. La transformación se aplica a cada imagen del dataset. Por lo tanto si aplico N veces el proceso obtengo N + 1 veces imágenes en total.\n",
    "\n",
    "### Blanco y Negro\n",
    "Dado que las imágenes son a color (RGB), se propone la alternativa de trabajar en blanco y negro, i.e. reducir el número de canales de 3 a 1. De esta forma, se reduce la infromación a un tercio, lo cual lo hace bastante atractivo computacionalmente. Además, esta desición se basa en la hipótesis de que detectar a que clase pertenece cada carta no depende del color en sí, sino de la froma del número o símbolo. En la misma línea, la simbología casi en todos los casos está pintada de un solo color, por lo que podemos pensar que el color en sí no es una variable predominante en la clasificación. Por ende, está hipótesis será puesta a prueba en la resolución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolución: Color & Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición de parámetros principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_rgb_dag = {\n",
    "        'conv_layers': [\n",
    "            {\n",
    "                'in_channels': 3,\n",
    "                'out_channels': 16,\n",
    "                'kernel_size': 3,\n",
    "                'stride': 1,\n",
    "                'padding': 1,\n",
    "                'padding_mode': 'zeros',\n",
    "                'pool_kernel_size': 2,\n",
    "                'pool_stride': 2,\n",
    "                'pool_padding': 0\n",
    "            },\n",
    "            {\n",
    "                'in_channels': 16,\n",
    "                'out_channels': 32,\n",
    "                'kernel_size': 3,\n",
    "                'stride': 1,\n",
    "                'padding': 1,\n",
    "                'padding_mode': 'zeros',\n",
    "                'pool_kernel_size': 2,\n",
    "                'pool_stride': 2,\n",
    "                'pool_padding': 0\n",
    "            },\n",
    "            {\n",
    "                'in_channels': 32,\n",
    "                'out_channels': 64,\n",
    "                'kernel_size': 3,\n",
    "                'stride': 1,\n",
    "                'padding': 1,\n",
    "                'padding_mode': 'zeros',\n",
    "                'pool_kernel_size': 2,\n",
    "                'pool_stride': 2,\n",
    "                'pool_padding': 0\n",
    "            },\n",
    "        ],\n",
    "        'full_layers': [128],\n",
    "        'n_classes': 53,\n",
    "        'dropout_rate': 0.2,\n",
    "        'activation': nn.ReLU(),\n",
    "        'criterion': nn.CrossEntropyLoss(),\n",
    "        'init_type': 'xavier',\n",
    "        'device': device\n",
    "    }\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['b', 'r', 'g', 'm', 'c']\n",
    "batch_size = [128, 64, 64]\n",
    "data_aug_loops = [0, 1, 2, 3, 4]\n",
    "model_files = [f\"rgb_dag_{n}.pth\" for n in data_aug_loops]\n",
    "metric_files = [f\"rgb_dag_{n}.txt\" for n in data_aug_loops]\n",
    "datasets_rgb_dag = []\n",
    "models_rgb_dag = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fit parameters.\n",
    "optimizer = optim.Adam\n",
    "epochs = 5\n",
    "lr = 1e-4\n",
    "verbose = True\n",
    "epoch_print = 1\n",
    "tolerance = 1e-3\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creación los dataset y construcción de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dag_loop, model_file, metric_file in zip(\n",
    "    data_aug_loops, model_files, metric_files):\n",
    "    # Create the dataset\n",
    "    cards = Cards(\n",
    "        transform=transform,\n",
    "        train_path=TRAIN_PATH, \n",
    "        valid_path=VALID_PATH, \n",
    "        test_path=TEST_PATH, \n",
    "        device=device\n",
    "    )\n",
    "    if dag_loop > 0:\n",
    "        cards.augmentDataset(loops=dag_loop, join=True)\n",
    "    cards.getDatasets()\n",
    "    cards.getDataloaders(batch_size=batch_size)\n",
    "    datasets_rgb_dag.append(cards)\n",
    "\n",
    "    # Build the model\n",
    "    conv_blocks = []\n",
    "    for conv_layer in params_rgb_dag['conv_layers']:\n",
    "        conv_block = ConvolutionalBlock(**conv_layer)\n",
    "        conv_blocks.append(conv_block)\n",
    "    model = CNN(conv_blocks=conv_blocks, \n",
    "                image_shape=(cards.img_channels, cards.img_width, cards.img_height),\n",
    "                n_classes=params_rgb_dag['n_classes'],\n",
    "                out_neurons=params_rgb_dag['full_layers'][0],\n",
    "                activation=params_rgb_dag['activation'], \n",
    "                criterion=params_rgb_dag['criterion'], \n",
    "                dropout_rate=params_rgb_dag['dropout_rate'],\n",
    "                init_type=params_rgb_dag['init_type'],\n",
    "                device=params_rgb_dag['device'])\n",
    "    model = model.to(model.device)\n",
    "\n",
    "    # Load pre trained model if exists\n",
    "    model_file = MODELS_PATH + f\"base/{model_file}\"\n",
    "    metric_file = METRICS_PATH + f\"base/{metric_file}\"\n",
    "    if os.path.exists(model_file) and os.path.exists(metric_file):\n",
    "        model.load(model_file, metric_file)\n",
    "    models_rgb_dag.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cards, model, model_file, metric_file in zip(\n",
    "    datasets_rgb_dag, models_rgb_dag, model_files, metric_files):\n",
    "    model.fit(train_dataloader=cards.train_dataloader, \n",
    "            optimizer=optimizer, \n",
    "            epochs=epochs, \n",
    "            lr=lr, \n",
    "            val_dataloader=cards.valid_dataloader, \n",
    "            verbose=verbose, \n",
    "            epoch_print=epoch_print, \n",
    "            tolerance=tolerance, \n",
    "            patience=patience)\n",
    "    model_file = MODELS_PATH + f\"base/{model_file}\"\n",
    "    metric_file = METRICS_PATH + f\"base/{metric_file}\"\n",
    "    model.save(model_path=model_file, metrics_path=metric_file)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gráfica de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Plot loss\n",
    "for model, dag_loop, color in zip(models_rgb_dag, data_aug_loops, colors):\n",
    "    epochs_vector = model.metrics['epochs']\n",
    "    train_loss = model.metrics['loss']['train']\n",
    "    val_loss = model.metrics['loss']['val']\n",
    "    label = f\"Train - DAG x{dag_loop}\" if dag_loop > 0 else \"Train\"\n",
    "    axs[0].plot(epochs_vector, train_loss, f\"{color}-\",label=label)\n",
    "    axs[0].plot(epochs_vector, val_loss, f\"{color}--\")\n",
    "axs[0].set_xlabel(\"epochs\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].legend(loc='best')\n",
    "axs[0].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Plot accuracy\n",
    "for model, dag_loop, color in zip(models_rgb_dag, data_aug_loops, colors):\n",
    "    epochs_vector = model.metrics['epochs']\n",
    "    train_acc = [100*acc for acc in model.metrics['accuracy']['train']]\n",
    "    val_acc = [100*acc for acc in model.metrics['accuracy']['val']]\n",
    "    label = f\"Train - DAG x{dag_loop}\" if dag_loop > 0 else \"Train\"\n",
    "    axs[1].plot(epochs_vector, train_acc, f\"{color}-\",label=label)\n",
    "    axs[1].plot(epochs_vector, val_acc, f\"{color}--\")\n",
    "axs[1].set_xlabel(\"epochs\")\n",
    "axs[1].set_ylabel(\"Accuracy [%]\")\n",
    "axs[1].legend(loc='best')\n",
    "axs[1].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.suptitle(\"Métricas - RGB - Data Augmentation\")\n",
    "plt.tight_layout()\n",
    "filename = FIGURES_PATH + f\"base/metrics_rgb_dag.png\"\n",
    "plt.savefig(filename, dpi=100, facecolor='w', edgecolor='w')\n",
    "#plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolución: Blanco y Negro & Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición de parámetros principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_bw_dag = {\n",
    "        'conv_layers': [\n",
    "            {\n",
    "                'in_channels': 1,\n",
    "                'out_channels': 16,\n",
    "                'kernel_size': 3,\n",
    "                'stride': 1,\n",
    "                'padding': 1,\n",
    "                'padding_mode': 'zeros',\n",
    "                'pool_kernel_size': 2,\n",
    "                'pool_stride': 2,\n",
    "                'pool_padding': 0\n",
    "            },\n",
    "            {\n",
    "                'in_channels': 16,\n",
    "                'out_channels': 32,\n",
    "                'kernel_size': 3,\n",
    "                'stride': 1,\n",
    "                'padding': 1,\n",
    "                'padding_mode': 'zeros',\n",
    "                'pool_kernel_size': 2,\n",
    "                'pool_stride': 2,\n",
    "                'pool_padding': 0\n",
    "            },\n",
    "            {\n",
    "                'in_channels': 32,\n",
    "                'out_channels': 64,\n",
    "                'kernel_size': 3,\n",
    "                'stride': 1,\n",
    "                'padding': 1,\n",
    "                'padding_mode': 'zeros',\n",
    "                'pool_kernel_size': 2,\n",
    "                'pool_stride': 2,\n",
    "                'pool_padding': 0\n",
    "            },\n",
    "        ],\n",
    "        'full_layers': [128],\n",
    "        'n_classes': 53,\n",
    "        'dropout_rate': 0.2,\n",
    "        'activation': nn.ReLU(),\n",
    "        'criterion': nn.CrossEntropyLoss(),\n",
    "        'init_type': 'xavier',\n",
    "        'device': device\n",
    "    }\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['b', 'r', 'g']\n",
    "batch_size = [128, 64, 64]\n",
    "data_aug_loops = [0, 1, 2]\n",
    "model_files = [f\"bw_dag_{n}.pth\" for n in data_aug_loops]\n",
    "metric_files = [f\"bw_dag_{n}.txt\" for n in data_aug_loops]\n",
    "datasets_bw_dag = []\n",
    "models_bw_dag = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fit parameters.\n",
    "optimizer = optim.AdamW\n",
    "epochs = 10\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-5\n",
    "verbose = True\n",
    "epoch_print = 1\n",
    "tolerance = 1e-3\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creación los dataset y construcción de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for da_loop, model_file, metric_file in zip(\n",
    "    data_aug_loops, model_files, metric_files):\n",
    "    # Create the dataset\n",
    "    cards = Cards(\n",
    "        transform=transform,\n",
    "        train_path=TRAIN_PATH, \n",
    "        valid_path=VALID_PATH, \n",
    "        test_path=TEST_PATH, \n",
    "        device=device\n",
    "    )\n",
    "    if da_loop > 0:\n",
    "        cards.augmentDataset(loops=da_loop, join=True)\n",
    "    cards.turnDataset2GrayScale()\n",
    "    cards.getDatasets()\n",
    "    cards.getDataloaders(batch_size=batch_size)\n",
    "    datasets_bw_dag.append(cards)\n",
    "\n",
    "    # Build the model\n",
    "    conv_blocks = []\n",
    "    for conv_layer in params_bw_dag['conv_layers']:\n",
    "        conv_block = ConvolutionalBlock(**conv_layer)\n",
    "        conv_blocks.append(conv_block)\n",
    "    model = CNN(conv_blocks=conv_blocks, \n",
    "                image_shape=(cards.img_channels, cards.img_width, cards.img_height),\n",
    "                n_classes=params_bw_dag['n_classes'],\n",
    "                out_neurons=params_bw_dag['full_layers'][0],\n",
    "                activation=params_bw_dag['activation'], \n",
    "                criterion=params_bw_dag['criterion'], \n",
    "                dropout_rate=params_bw_dag['dropout_rate'],\n",
    "                init_type=params_bw_dag['init_type'],\n",
    "                device=params_bw_dag['device'])\n",
    "    model = model.to(model.device)\n",
    "\n",
    "    # Load pre trained model if exists\n",
    "    model_file = MODELS_PATH + f\"base/{model_file}\"\n",
    "    metric_file = METRICS_PATH + f\"base/{metric_file}\"\n",
    "    if os.path.exists(model_file) and os.path.exists(metric_file):\n",
    "        model.load(model_file, metric_file)\n",
    "    models_bw_dag.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cards, model, model_file, metric_file in zip(\n",
    "    datasets_bw_dag, models_bw_dag, model_files, metric_files):\n",
    "    model.fit(train_dataloader=cards.train_dataloader, \n",
    "            optimizer=optimizer, \n",
    "            epochs=epochs, \n",
    "            lr=lr,\n",
    "            regularization=weight_decay, \n",
    "            val_dataloader=cards.valid_dataloader, \n",
    "            verbose=verbose, \n",
    "            epoch_print=epoch_print, \n",
    "            tolerance=tolerance, \n",
    "            patience=patience)\n",
    "    model_file = MODELS_PATH + f\"base/{model_file}\"\n",
    "    metric_file = METRICS_PATH + f\"base/{metric_file}\"\n",
    "    model.save(model_path=model_file, metrics_path=metric_file)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gráfica de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Plot loss\n",
    "for model, dag_loop, color in zip(models_bw_dag, data_aug_loops, colors):\n",
    "    epochs_vector = model.metrics['epochs']\n",
    "    train_loss = model.metrics['loss']['train']\n",
    "    val_loss = model.metrics['loss']['val']\n",
    "    label = f\"Train - DAG x{dag_loop}\" if dag_loop > 0 else \"Train\"\n",
    "    axs[0].plot(epochs_vector, train_loss, f\"{color}-\",label=label)\n",
    "    axs[0].plot(epochs_vector, val_loss, f\"{color}--\")\n",
    "axs[0].set_xlabel(\"epochs\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].legend(loc='best')\n",
    "axs[0].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Plot accuracy\n",
    "for model, dag_loop, color in zip(models_bw_dag, data_aug_loops, colors):\n",
    "    epochs_vector = model.metrics['epochs']\n",
    "    train_acc = [100*acc for acc in model.metrics['accuracy']['train']]\n",
    "    val_acc = [100*acc for acc in model.metrics['accuracy']['val']]\n",
    "    label = f\"Train - DAG x{dag_loop}\" if dag_loop > 0 else \"Train\"\n",
    "    axs[1].plot(epochs_vector, train_acc, f\"{color}-\",label=label)\n",
    "    axs[1].plot(epochs_vector, val_acc, f\"{color}--\")\n",
    "axs[1].set_xlabel(\"epochs\")\n",
    "axs[1].set_ylabel(\"Accuracy [%]\")\n",
    "axs[1].legend(loc='best')\n",
    "axs[1].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.suptitle(\"Métricas - B&W - Data Augmentation\")\n",
    "plt.tight_layout()\n",
    "filename = FIGURES_PATH + f\"base/metrics_bw_dag.png\"\n",
    "plt.savefig(filename, dpi=100, facecolor='w', edgecolor='w')\n",
    "#plt.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build encoder\n",
    "encoder = Autoencoder(device=device)\n",
    "encoder = encoder.to(encoder.device)\n",
    "\n",
    "# Temporarily move autoencoder to CPU for the summary\n",
    "encoder_cpu = encoder.to(\"cpu\")\n",
    "torchsummary.summary(encoder_cpu, (cards_aug.img_channels, cards_aug.img_width, cards_aug.img_height), device=\"cpu\")\n",
    "# Move the autoencoder back to MPS after summary (if MPS was available)\n",
    "encoder = encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define fit parameters for autoencoder.\n",
    "optimizer = optim.Adam\n",
    "epochs = 40\n",
    "lr = 1e-4\n",
    "verbose = True\n",
    "epch_print = 5\n",
    "tolerance = 1e-3\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the autoencoder\n",
    "encoder.fit(train_dataloader=cards_aug.train_dataloader, \n",
    "            optimizer=optimizer, \n",
    "            epochs=epochs, \n",
    "            lr=lr, \n",
    "            eval_dataloader=cards_aug.valid_dataloader, \n",
    "            verbose=verbose, \n",
    "            epch_print=epch_print, \n",
    "            tolerance=tolerance, \n",
    "            patience=patience)\n",
    "\n",
    "# Save the autoencoder\n",
    "model_path = MODELS_PATH + \"autoencoder.pth\"\n",
    "metrics_path = METRICS_PATH + \"autoencoder.txt\"\n",
    "encoder.save(model_path=model_path, metrics_path=metrics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, _ in cards_aug.train_data:\n",
    "    image = image.unsqueeze(0)\n",
    "    encoder.showEncodedImage(image)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "weights = ResNet18_Weights.IMAGENET1K_V1\n",
    "transforms = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18_model = resnet18(weights=weights)\n",
    "resnet18_model = resnet18_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet18_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "last_layer_in_features = resnet18_model.fc.in_features\n",
    "resnet18_model.fc = torch.nn.Linear(in_features=last_layer_in_features, out_features=params_rgb_dag[\"n_classes\"], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLoss(model, dataloader):\n",
    "        loss = 0.0\n",
    "        samples = 0\n",
    "        model.eval() # set the model to evaluation mode\n",
    "        with torch.no_grad():   \n",
    "            for inputs_batch, targets_batch in dataloader:\n",
    "                batch_size = inputs_batch.size(0)\n",
    "                samples += batch_size\n",
    "                if device != 'cpu':\n",
    "                    inputs_batch = inputs_batch.to(device)\n",
    "                    targets_batch = targets_batch.to(device)\n",
    "                predictions_batch = model(inputs_batch)\n",
    "                loss += params_rgb_dag[\"criterion\"](predictions_batch, targets_batch).item() * batch_size\n",
    "        return loss / samples\n",
    "    \n",
    "def computeAccuracy(model, dataloader):\n",
    "    correct, total = 0, 0\n",
    "    model.eval() # set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for inputs_batch, targets_batch in dataloader:\n",
    "            if device != 'cpu':\n",
    "                inputs_batch = inputs_batch.to(device)\n",
    "                targets_batch = targets_batch.to(device)\n",
    "            predictions_batch = model(inputs_batch)\n",
    "            correct += (predictions_batch.argmax(dim=1) == targets_batch).sum().item()\n",
    "            total += inputs_batch.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_dataloader, optimizer=optim.Adam, epochs=30, lr=1e-4, \n",
    "        regularization=0.0, val_dataloader=None, verbose=True, epoch_print=1):\n",
    "    \n",
    "        # Set the optimizer\n",
    "        optimizer = optimizer(model.parameters(), lr=lr, weight_decay=regularization)\n",
    "\n",
    "        # Start the training\n",
    "        for i in range(epochs):\n",
    "            model.train()\n",
    "            for inputs_batch, targets_batch in train_dataloader:\n",
    "                if device != 'cpu':\n",
    "                    inputs_batch = inputs_batch.to(device)\n",
    "                    targets_batch = targets_batch.to(device)\n",
    "                predictions_batch = model(inputs_batch)\n",
    "                loss_batch = params_rgb_dag[\"criterion\"](predictions_batch, targets_batch)\n",
    "                optimizer.zero_grad()\n",
    "                loss_batch.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Evaluate the model\n",
    "            model.eval()\n",
    "            train_loss = computeLoss(model, train_dataloader)\n",
    "            train_acc = computeAccuracy(model, train_dataloader)\n",
    "            if val_dataloader:\n",
    "                eval_loss = computeLoss(model, val_dataloader)\n",
    "                eval_acc = computeAccuracy(model, val_dataloader)\n",
    "            \n",
    "            # Print the progress\n",
    "            if verbose and (i + 1) % epoch_print == 0:\n",
    "                eval_loss = eval_loss if val_dataloader else 'N/A'\n",
    "                text = f\"Epoch {i + 1}/{epochs}: \"\n",
    "                text += f\"Loss ({train_loss:.4g}, {eval_loss:.4g}) \\t \"\n",
    "                text += f\"Accuracy ({100*train_acc:.2f}%, {100*eval_acc:.2f}%)\"\n",
    "                print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(resnet18_model, datasets_rgb_dag[0].train_dataloader, epochs=5, \n",
    "    lr=1e-3, val_dataloader=datasets_rgb_dag[0].valid_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
